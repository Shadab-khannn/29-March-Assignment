{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554e6962-b086-412a-91ab-6f3ca2b86457",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Lasso Regression, and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fb3662-dde0-4564-91fc-5dd97d511c94",
   "metadata": {},
   "source": [
    "ANS-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b95a8d-84ec-448a-bc3e-e7a56baf81ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lasso Regression is a regression analysis method that performs both variable selection and regularization. It uses absolute values in \n",
    "the penalty function instead of squares, which leads to penalizing (or equivalently constraining the sum of the absolute values of the\n",
    "estimates) values that cause some of the parameter estimates to turn out exactly zero. This makes Lasso Regression useful for feature\n",
    "selection.\n",
    "\n",
    "Lasso Regression is similar to Ridge Regression in that both attempt to minimize the sum of squared residuals (RSS) along with some penalty\n",
    "term. However, Lasso Regression constrains or regularizes the coefficient estimates of the model by zeroing out some features’ coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1640754-97dd-4f78-903d-492844429d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0724f52c-5127-454d-b4ef-3a1b86432cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is the main advantage of using Lasso Regression in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2364ed6d-da80-4357-9bed-1d07806fbd57",
   "metadata": {},
   "source": [
    "ANS-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440af92a-373f-4e21-97b0-aff91ae0fcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lasso regression is useful for feature selection because it provides a principled way to reduce the number of features in a model. \n",
    "By adding the L1 regularization term, Lasso regression can shrink the coefficients towards zero, and when λ is sufficiently large, some \n",
    "coefficients are driven to exactly zero. This property of Lasso makes it useful for feature selection, as the variables with zero coefficients \n",
    "are effectively removed from the model. Lasso was designed to improve the interpretability of machine learning models by reducing the number\n",
    "of features.\n",
    "\n",
    "The main advantage of a LASSO regression model is that it has the ability to set the coefficients for features it does not consider\n",
    "interesting to zero. This means that the model does some automatic feature selection to decide which features should and should not be \n",
    "included on its own. Reduced overfitting is another advantage of LASSO regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9611d74f-29f4-4715-ab0f-01542f49d07b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a711a0b-4253-4ccc-9f82-13f340f6ad7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How do you interpret the coefficients of a Lasso Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336628c3-1aa1-48c3-a799-2e5a559c446e",
   "metadata": {},
   "source": [
    "ANS-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec8240e-05c3-4abf-9e27-352380747890",
   "metadata": {},
   "outputs": [],
   "source": [
    "In Lasso Regression model, the coefficients are interpreted as the log odds for a 1 unit change in the coefficient while holding all other\n",
    "coefficients constant. Lasso regression performs L1 regularization, which adds a penalty equal to the absolute value of the magnitude of\n",
    "coefficients. This type of regularization can result in sparse models with few coefficients; Some coefficients can become zero and eliminated\n",
    "from the model. The coefficients can be used to understand the impact of each feature on the target variable, and also help in feature\n",
    "selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f4b2ae-a5cf-416b-9ef7-e68bc9eae8b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9217d73-ca6e-49a3-b40e-d7dbdf983956",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the\n",
    "model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482bb5ef-1613-4777-a70c-1389415be131",
   "metadata": {},
   "source": [
    "ANS-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0379954-a09f-4119-9a2b-f3e39082363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "In Lasso Regression, the tuning parameter (λ), sometimes called a penalty parameter, controls the strength of the penalty term in \n",
    "ridge regression and lasso regression. It is basically the amount of shrinkage, where data values are shrunk towards a central point, \n",
    "like the mean. The tuning parameter is used to control the trade-off between fitting the model well to the training data and keeping \n",
    "the model simple. A larger value of λ will result in more shrinkage and a simpler model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30299941-f707-4b80-8f1a-196f3e242b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e785edf4-f38a-4044-b790-95e3ec32f81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf3aa05-9477-47ff-b288-1df6a88a5e33",
   "metadata": {},
   "source": [
    "ANS-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5529dd6e-b82e-4609-9f81-e62087726a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "Yes, Lasso Regression can be used for non-linear regression problems. One way to do this is by using Gaussian basis functions and imposing \n",
    "a weighted lasso penalty on a nonlinear regression model. This can help select the number of basis functions effectively and reduce some \n",
    "unknown parameters in linear regression models toward exactly zero.\n",
    "\n",
    "Another way is to fit a lasso regressor to the whole lot, multiplying out your brackets giving you 2m+2 coefficients. Then by performing a \n",
    "change of variables you can make this a linear regression problem again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d42b5b-12e0-40f2-84db-89aa74d2bf6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0688fe01-e768-44dd-ac8a-dd81fc4cbc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. What is the difference between Ridge Regression and Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5212e28-783a-418b-bb31-b87864eb9a02",
   "metadata": {},
   "source": [
    "ANS-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8c44a6-2e35-4aac-9f00-04f4ebf3bc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Both Ridge Regression and Lasso Regression are regularization methods that minimize the sum of squared residuals along with some penalty term.\n",
    "The difference is that Ridge Regression takes the square of the coefficients and never sets them to absolute zero, while Lasso Regression\n",
    "takes the magnitude and tends to make coefficients to absolute zero. Lasso Regression can be used for automatic feature selection and removing \n",
    "insignificant variables from the model. However, Lasso Regression may struggle with some types of data, such as when the number of predictors\n",
    "is greater than the number of observations, or when there are highly collinear variables. In these cases, Elastic Net, which combines the\n",
    "regularization of both Lasso and Ridge, may be better. Ridge Regression may perform better when many predictor variables are significant \n",
    "and have similar coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7f6ccb-f26a-414d-8da6-2a50ef6a088a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c311b4-90e4-44f8-913b-24036578bec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18cdc78-cb6b-442d-af21-609dffb66a36",
   "metadata": {},
   "source": [
    "ANS-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1152ad3b-dbbc-42a5-8f6e-4d64b6b5567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Yes, Lasso Regression can handle multicollinearity in the input features. In fact, Lasso Regression is one of the methods that can handle\n",
    "multicollinearity. It does this by shrinking the coefficients of correlated variables towards zero1. This is because Lasso Regression uses\n",
    "L1 regularization which adds a penalty term to the loss function that is proportional to the absolute value of the coefficients.\n",
    "This penalty term forces some of the coefficients to be zero, which effectively removes some of the features from the model. \n",
    "This way, Lasso Regression can handle multicollinearity by selecting only one feature from a group of correlated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ef66d8-dae3-45e5-a677-04e92f4f67d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3950656f-2e4b-43f4-9af8-24a34850d900",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5417e707-09f8-4498-828e-086db2689753",
   "metadata": {},
   "source": [
    "ANS-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb2df52-56bf-434f-98e7-123bed45b3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "To choose the optimal value of the regularization parameter (lambda) in Lasso Regression, there are two main approaches:\n",
    "\n",
    "Choose λ such that some information criterion, e.g., AIC or BIC, is the smallest.\n",
    "Perform cross-validation and select the value of λ that minimizes the cross-validated sum of squared residuals (or some other measure).\n",
    "Alternatively, you can use root finding algorithms like Brent’s method or golden section search to directly optimize λ ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
